# AGENTS.md

AI coding agent instructions for **modellm**

## Project Overview

**Project Type:** Python Application
**Primary Language:** Python (100% of codebase)

## Key Files

**Documentation:**
- `developer_todo.md` -- master (human engineer) todo list
- `agents/implementation_guide.md` -- Detailed implementation guide with specifications for tasks
- `agents/tools/README.md` -- Context management tools and workflow
- `doc/migration_proposal.md` -- Comprehensive implementation plan for prompt pipeline
- `doc/workflow_guide.md` -- Usage workflow and examples
- `doc/typedb_schema_2.tql` -- TypeDB schema (v3 syntax)
- `doc/typedb_http_api.md` -- guide to the HTTP api
- `doc/TypeQL3_REF.md` -- TypeQL version 3 reference

**Configuration:**
- `configuration/pipeline_config.yaml` -- Flexible step configuration for prompt pipeline
- `pyproject.toml` -- Updated with prompt-pipeline CLI tool

**Prompt Pipeline Library:**
- `prompt_pipeline/llm_client.py` -- OpenRouter API client
- `prompt_pipeline/prompt_manager.py` -- Prompt loading and step configuration
- `prompt_pipeline/validation/` -- YAML and JSON validators
- `prompt_pipeline/step_executor.py` -- Individual step execution
- `prompt_pipeline/orchestrator.py` -- Pipeline coordination

**CLI Tool:**
- `prompt_pipeline_cli/main.py` -- Main CLI entry point
- `prompt_pipeline_cli/commands/` -- CLI command implementations

**Prompts:**
- `prompts/` -- Prompt templates for pipeline steps (step1, step2, step3, stepC3, stepC4, stepC5, stepD1)

**Agent Tools:**
- `agents/tools/extract_context.py` -- Extract minimal context for a task
- `agents/tools/build_task_context.py` -- Build named context files
- `agents/tools/check_context_size.py` -- Analyze context window usage
- `agents/tools/cli_syntax_checker.py` -- Validate CLI syntax and get hints
- `agents/tools/workflow_guide.md` -- Standardized workflow patterns
- `agents/tools/README.md` -- Tool usage guide
- `agents/context/` -- Generated context files (task-specific)

## Context Management System

### **IMPORTANT: Context Window Guidelines**

The implementation guide (`agents/implementation_guide.md`) is large and can exceed context window limits. **NEVER copy-paste the entire guide**. Instead:

1. **Extract minimal context** before starting each task
2. **Reference the guide by section** (e.g., "See implementation_guide.md section 2.7 for CLI inputs")
3. **Use context files** generated by the tools in `agents/tools/`

### Tool-Based Context Workflow

```bash
# Step 1: Extract minimal context
python agents/tools/extract_context.py \
  --task "Add tag replacement system" \
  --files prompt_pipeline/prompt_manager.py:substitute_variables \
          prompt_pipeline/step_executor.py:execute_step

# Step 2: Check context size
python agents/tools/check_context_size.py \
  --context-file agents/context/task_xyz.txt \
  --model gpt-4o

# Step 3: If context is CRITICAL or WARNING, STOP and ask for task split
```

### When to Stop and Split Tasks

**STOP the task immediately if:**
1. Context size analysis shows CRITICAL or WARNING status
2. Context file shows >70% of model limit
3. You're working on >3 files at once
4. You're stuck and context might be the issue
5. The human engineer asks you to stop

**How to stop:**
1. Create a message explaining what you've completed and what remains
2. Update task status (use `update_task` or indicate in message)
3. Provide the context file path for review
4. Wait for engineer to provide direction

### Context Size Rules

| Status | Token Usage | Action |
|--------|-------------|--------|
| OK | < 70% of limit | Proceed with task |
| CAUTION | 70-80% | Continue, but be mindful |
| WARNING | 80-90% | Stop and ask for task split |
| CRITICAL | > 90% | STOP immediately |

## Task Management
## Task Creation *IMPORTANT*
- **Keep tasks as narrow as possible** to minimize required context
- **Reason carefully** (ðŸ’¡) taking into account complexity and dependencies
- **It is better to have 30 tasks implementing one function each than 1 task implementing 30 functions**
- **Each task should be a small testable, verifiable, atomic unit** (as practically possible)
- **If scope expands beyond one module or 2-3 functions, verify the plan with the human engineer**
- **Before creating tasks, extract minimal context** using `agents/tools/extract_context.py`

### Before Starting Any Task

**Follow the 5-Step Workflow (see agents/tools/workflow_guide.md for details):**

1. **GET HINTS**: Check syntax before doing anything
   ```bash
   python agents/tools/cli_syntax_checker.py --tool extract_context --examples
   ```

2. **VALIDATE**: Check command syntax (no execution)
   ```bash
   python agents/tools/cli_syntax_checker.py --tool extract_context --validate \
     --task "Your task" --files prompt_pipeline/file.py:function
   ```

3. **EXTRACT**: Get minimal context
   ```bash
   python agents/tools/extract_context.py \
     --task "Your task description" \
     --files prompt_pipeline/specific_file.py:specific_function
   ```

4. **CHECK SIZE**: Ensure context is manageable
   ```bash
   python agents/tools/check_context_size.py \
     --context-file agents/context/task_xyz.txt \
     --model gpt-4o
   ```

5. **DECIDE**: 
   - If status is **OK** (< 70%): Proceed with task
   - If status is **WARNING** or **CRITICAL**: STOP and ask for task split

**Additional steps:**
- Review line numbers in the task description
- Check existing code before making changes
- Understand context from extracted context file
- Verify test requirements

### Task Reference System
All implementation tasks are tracked in `.nanocoder/tasks.json` and reference specific sections in `agents/implementation_guide.md`. Each task includes:
- **Priority level**: CRITICAL, HIGH, or MEDIUM
- **File paths**: Exact file locations to modify
- **Line numbers**: Specific line ranges referenced
- **Implementation guide section**: Links to detailed specifications
- **Test requirements**: Detailed test specifications
- **Success criteria**: Clear completion criteria

### During Task Implementation
- Follow the implementation specifications in the guide
- Add type hints to all new functions
- Include comprehensive docstrings
- Write tests BEFORE implementation (TDD approach)
- Use small, focused commits
- Test with the actual TypeDB server when possible

### **STOPPING PROCEDURE (CRITICAL)**
If context size becomes too large or you get stuck:

1. **Check context size**
   ```bash
   python agents/tools/check_context_size.py \
     --context-file agents/context/task_<id>.txt \
     --model gpt-4o
   ```

2. **If status is CRITICAL or WARNING**:
   - STOP the task immediately
   - Do NOT continue working
   - Create a message explaining:
     - What you've completed
     - What remains to be done
     - Why you're stopping (context size, blocked, etc.)
   - Provide the context file path for review

3. **Wait for engineer direction**
   - The engineer may split the task
   - The engineer may provide clarification
   - Do NOT proceed until you get direction

### After Task Completion
- All tests must pass (`pytest tests/ -v`)
- Security tests pass with malicious inputs
- Performance benchmarks show improvement
- Code coverage > 90% for new code
- No deprecation warnings when running

## Code Style Guidelines
- DO NOT USE inline imports. Imports belong at the top of the module.
- Follow PEP 8 style guide
- Use snake_case for variables and functions
- Use PascalCase for classes
- Include docstrings for functions and classes
- Use type hints where appropriate
- Clean up the directories of test scripts and temporary use tools when tasks are completed.
  - If tools are useful enough to use again save them to the `agents/tools` or `agents/tests` folder.
  - Note unit tests and validations should be saved elsewhere

## Testing
- All code needs at least 80% test coverage
- Unit tests AND integration tests (against test server) are required
- All newly generated code needs to pass all tests before the task can be considered complete
- There is a TypeQL test database server at `http://localhost:8000` creds: admin/password
  - It is a docker container for test purposes

```bash
# Run all tests
pytest tests/ -v

# Run specific test file
pytest tests/test_client.py -v

# Run with coverage
pytest tests/ --cov=tools/typedb_v3_client --cov-report=html

# Run performance benchmarks
pytest tests/performance_tests.py -v --benchmark-only
```

### Viewing Implementation Guide
```bash
# View specific section
head -60 agents/implementation_guide.md  # Task 1 spec
head -135 agents/implementation_guide.md | tail -75  # Task 2-3 specs
```

### Task Management
```bash
# View all tasks
list_tasks

# Update task status
update_task --id <task_id> --status in_progress

# Mark task complete
update_task --id <task_id> --status completed
```

## AI Coding Assistance Notes
- The `./agents` folder is for all things to do with LLM agents ONLY; this folder is not for project deliverables
- The `agents/tools/` folder contains helper utilities for context management
- The `agents/context/` folder stores generated context files (automatically created)

**Important Considerations:**
- Check virtual environment setup before running commands
- Be mindful of Python version compatibility
- Follow import organization (stdlib, third-party, local)
- Project has 38 files across 4 directories

---

*This AGENTS.md file was generated by Nanocoder. Update it as your project evolves.*
